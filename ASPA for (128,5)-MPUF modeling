# The code of ASPA for (128,5)-MPUF modeling
# hierarchy architecture; original auxiliary task 
import numpy as np
import keras
import keras_metrics
import pandas as pd
from keras import layers 
from keras.layers import Input,Embedding,LSTM,Dense,Dropout
from keras.models import Model
from keras.callbacks import EarlyStopping
from keras.callbacks import TensorBoard 
from sklearn import model_selection
from sklearn.model_selection import train_test_split

tbCallBack = keras.callbacks.TensorBoard(log_dir='./Graph/(128,5)MPUF', 
histogram_freq=1, 
write_graph=True, 
write_images=True)         

# load training data
df_challenge=pd.read_csv('./multitask/MPUF/o_(128,5)MPUF.csv',header=None)            # challenges of PUF 
df_response=pd.read_csv('./multitask/MPUF/r_(128,5)MPUF.csv',header=None)             # responses of PUF
df_power_level=pd.read_csv('./multitask/MPUF/power_(128,5)MPUF.csv',header=None)      # power level of PUF

X = df_challenge.iloc[:600000,:129]
Y = df_response.iloc[:600000,:1]
Z = df_power_level.iloc[:600000,:1]

# create model ; implemented with hard parameter sharing architecture, generated auxiliary task.
w = 1                                                                   # loss weight ratio
power_level_number=38                                                   # number of classes, corresponding to power level
main_input=Input(shape=(129,),dtype='int32',name='main_input')
main_output=Input(shape=(1,),dtype='bool',name='main_output')
auxiliary_output=Input(shape=(1,),dtype='int32',name='auxiliary_output')
x=Dense(100,activation='relu')(main_input)
x=Dense(100,activation='relu')(x)
x=Dense(100,activation='relu')(x)
y=Dense(100,activation='relu')(x)                                               # hierarchy architecture
x=Dense(100,activation='relu')(y)
main_output=Dense(1,activation='sigmoid',name='main_output')(x)                    # for response
auxiliary_output=Dense(power_level_number,activation='softmax',name='aux_output')(y)  # for power level

model=Model(inputs=[main_input],outputs=[main_output,auxiliary_output])
model.summary()

train_challenges,test_challenges,train_responses,test_responses,train_power_levels,test_power_levels = model_selection.train_test_split(X, Y, Z,test_size = 0.2, random_state = 42)

model.compile(optimizer='Adam',
              loss={'main_output':'binary_crossentropy','aux_output':'sparse_categorical_crossentropy'},
              loss_weights={'main_output':1 ,'aux_output': w },                                
              metrics=['accuracy',keras_metrics.precision(), keras_metrics.recall()]
)

results=model.fit({'main_input':train_challenges},{'main_output':train_responses,'aux_output':train_power_levels },
              epochs=200,batch_size=1000,
              validation_data = (test_challenges,[test_responses,test_power_levels]),
              callbacks = [EarlyStopping(patience=50,mode='min',verbose=1), tbCallBack]
              )       
# evaluate the model
scores =model.evaluate(test_challenges, [test_responses,test_power_levels]) 
print("\n%s: %.2f%%" % (model.metrics_names[3], scores[3]*100))
response_accuracy=scores[3]
power_level_accuracy=scores[6]
print(response_accuracy, power_level_accuracy)
